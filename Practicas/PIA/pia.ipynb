{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de los datos de entrenamiento: (9239, 10)\n",
      "Dimensiones de los datos de prueba: (2309, 9)\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos\n",
    "df_entrenamiento = pd.read_csv('train.csv')\n",
    "df_prueba = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"Dimensiones de los datos de entrenamiento:\", df_entrenamiento.shape)\n",
    "print(\"Dimensiones de los datos de prueba:\", df_prueba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de los datos con los que se entrenará: (7391, 8)\n",
      "Dimensiones de los datos de validación: (1848, 8)\n"
     ]
    }
   ],
   "source": [
    "# Convertir participación a valor numérico\n",
    "df_entrenamiento['engagement'] = df_entrenamiento['engagement'].astype(int)\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X = df_entrenamiento.drop(columns=['id', 'engagement'])\n",
    "y = df_entrenamiento['engagement']\n",
    "\n",
    "# Normalizar las características\n",
    "escalador = StandardScaler()\n",
    "X_escalada = escalador.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y de validación\n",
    "X_entrenamiento, X_val,y_entrenamiento, y_val \\\n",
    "    = train_test_split(X_escalada, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Dimensiones de los datos con los que se entrenará:\",X_entrenamiento.shape)\n",
    "print(\"Dimensiones de los datos de validación:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los modelos\n",
    "modelos = {\n",
    "    'Regresión logística': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'k vecinos más cercanos': KNeighborsClassifier(),\n",
    "    'Clasificador de margen máximo': SVC(probability=True, random_state=42),\n",
    "    'Árbol de decisión': DecisionTreeClassifier(),\n",
    "    'Bosque aleatorio': RandomForestClassifier(random_state=42),\n",
    "    'Potencialización de gradiente': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la cuadrícula de hiperparámetros\n",
    "cuadriculas_params = {\n",
    "    'Regresión logística': [\n",
    "        {\n",
    "            'C': [0.01, 0.02, 0.05, 0.1, 1],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        },\n",
    "        {\n",
    "            'C': [0.01, 0.02, 0.05, 0.1, 1],\n",
    "            'penalty': ['elasticnet'],\n",
    "            'solver': ['saga'],\n",
    "            'class_weight': [None, 'balanced'],\n",
    "            'l1_ratio': [0.1, 0.5, 0.9]\n",
    "        }\n",
    "    ],\n",
    "    'k vecinos más cercanos': {\n",
    "        'n_neighbors': [53, 55],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [2, 3],\n",
    "    },\n",
    "    'Clasificador de margen máximo': [\n",
    "        {\n",
    "            'C': [0.001, 0.01],\n",
    "            'kernel': ['rbf'],\n",
    "            'gamma': ['auto', 0.01]\n",
    "        },\n",
    "        {\n",
    "            'C': [0.001, 0.01],\n",
    "            'kernel': ['sigmoid'],\n",
    "            'gamma': ['auto', 0.001],\n",
    "            'coef0': [0.1, 1.0]\n",
    "        },\n",
    "        {\n",
    "            'C': [2, 3],\n",
    "            'kernel': ['poly'],\n",
    "            'gamma': [0.04, 0.05],\n",
    "            'degree': [2, 3],\n",
    "            'coef0': [13.0, 19.0]\n",
    "        }\n",
    "    ],\n",
    "    'Árbol de decisión': {\n",
    "        'max_depth': [5, None],\n",
    "        'min_samples_split': [5, 6],\n",
    "        'min_samples_leaf': [4, 5],\n",
    "        'max_features': [0.5, 0.8],\n",
    "        'criterion': ['entropy', 'log_loss'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'class_weight': ['balanced', None],\n",
    "        'max_leaf_nodes': [30, 40, None],\n",
    "        'min_weight_fraction_leaf': [0.0, 0.01],\n",
    "        'min_impurity_decrease': [0.0, 0.01]\n",
    "    },\n",
    "    'Bosque aleatorio': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'max_features': ['log2', 0.75],\n",
    "        'class_weight': ['balanced', None],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'Potencialización de gradiente': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [5, 10],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'loss': ['log_loss', 'exponential'],\n",
    "        'min_weight_fraction_leaf': [0.02, 0.03]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [97, 100],\n",
    "        'max_depth': [5, 10],\n",
    "        'learning_rate': [0.05,0.1],\n",
    "        'min_child_weight': [2, 5],\n",
    "        'subsample': [0.9, 1],\n",
    "        'gamma': [0.1, 0.2],\n",
    "        'reg_lambda': [0, 1],\n",
    "        'objective': ['binary:logistic', 'binary:logitraw'],\n",
    "        'monotone_constraints': [(1, 0), (1, -1)],\n",
    "        'reg_alpha': [0, 1],\n",
    "        'scale_pos_weight': [1, 2],\n",
    "        'booster': ['gbtree', 'dart']\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [300, 350],\n",
    "        'max_depth': [-1, 10],\n",
    "        'learning_rate': [0.01, 0.02],\n",
    "        'num_leaves': [30, 40],\n",
    "        'min_child_samples': [20, 30]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Regresión logística\n",
      "Fitting 5 folds for each of 70 candidates, totalling 350 fits\n",
      "Información guardada en Regresión logística.pkl\n",
      "Mejores hiperparámetros: {'C': 0.02, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}, ROC AUC (validación) = 0.8784\n",
      "Modelo: k vecinos más cercanos\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Información guardada en k vecinos más cercanos.pkl\n",
      "Mejores hiperparámetros: {'n_neighbors': 55, 'p': 2, 'weights': 'distance'}, ROC AUC (validación) = 0.8768\n",
      "Modelo: Clasificador de margen máximo\n",
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
      "Información guardada en Clasificador de margen máximo.pkl\n",
      "Mejores hiperparámetros: {'C': 3, 'coef0': 19.0, 'degree': 3, 'gamma': 0.04, 'kernel': 'poly'}, ROC AUC (validación) = 0.8907\n",
      "Modelo: Árbol de decisión\n",
      "Fitting 5 folds for each of 1536 candidates, totalling 7680 fits\n",
      "Información guardada en Árbol de decisión.pkl\n",
      "Mejores hiperparámetros: {'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 0.8, 'max_leaf_nodes': 40, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 5, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.01, 'splitter': 'best'}, ROC AUC (validación) = 0.8593\n",
      "Modelo: Bosque aleatorio\n",
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seelro06\\Downloads\\uanl-payroll-analysis\\uanl-payroll-analysis\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información guardada en Bosque aleatorio.pkl\n",
      "Mejores hiperparámetros: {'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 0.75, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}, ROC AUC (validación) = 0.8925\n",
      "Modelo: Potencialización de gradiente\n",
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seelro06\\Downloads\\uanl-payroll-analysis\\uanl-payroll-analysis\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información guardada en Potencialización de gradiente.pkl\n",
      "Mejores hiperparámetros: {'learning_rate': 0.1, 'loss': 'exponential', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.03, 'n_estimators': 100}, ROC AUC (validación) = 0.9045\n",
      "Modelo: XGBoost\n",
      "Fitting 5 folds for each of 4096 candidates, totalling 20480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seelro06\\Downloads\\uanl-payroll-analysis\\uanl-payroll-analysis\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información guardada en XGBoost.pkl\n",
      "Mejores hiperparámetros: {'booster': 'dart', 'gamma': 0.1, 'learning_rate': 0.05, 'max_depth': 5, 'min_child_weight': 2, 'monotone_constraints': (1, 0), 'n_estimators': 97, 'objective': 'binary:logistic', 'reg_alpha': 1, 'reg_lambda': 0, 'scale_pos_weight': 2, 'subsample': 0.9}, ROC AUC (validación) = 0.8995\n",
      "Modelo: LightGBM\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seelro06\\Downloads\\uanl-payroll-analysis\\uanl-payroll-analysis\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 732, number of negative: 6659\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1800\n",
      "[LightGBM] [Info] Number of data points in the train set: 7391, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.099039 -> initscore=-2.207944\n",
      "[LightGBM] [Info] Start training from score -2.207944\n",
      "Información guardada en LightGBM.pkl\n",
      "Mejores hiperparámetros: {'learning_rate': 0.01, 'max_depth': -1, 'min_child_samples': 30, 'n_estimators': 350, 'num_leaves': 40}, ROC AUC (validación) = 0.9031\n",
      "\n",
      "Modelo con el mejor rendimiento: GradientBoostingClassifier(loss='exponential', max_depth=5,\n",
      "                           min_weight_fraction_leaf=0.03, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# Evaluar los modelos\n",
    "mejor_puntaje = -float('inf')\n",
    "mejor_modelo = None\n",
    "resultados = {}\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    print(\"Modelo:\", nombre)\n",
    "\n",
    "    # Revisar si el modelo ya existe\n",
    "    if os.path.exists(f'{nombre}.pkl'):\n",
    "        with open(f'{nombre}.pkl', 'rb') as archivo:\n",
    "            datos = pickle.load(archivo)\n",
    "            modelo = datos['modelo']\n",
    "            parametros = datos['parametros']\n",
    "            puntaje = datos['puntaje']\n",
    "            print(\"Modelo cargado exitosamente.\")\n",
    "    else:\n",
    "        # Validación cruzada con el criterio de área bajo la curva característica operativa del receptor (ROC AUC)\n",
    "        busqueda = GridSearchCV(estimator=modelo, param_grid=cuadriculas_params[nombre], cv=5, scoring='roc_auc', n_jobs=-1, verbose=3)\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        busqueda.fit(X_entrenamiento, y_entrenamiento)\n",
    "        modelo = busqueda.best_estimator_\n",
    "\n",
    "        # Obtener los mejores hiperparámetros\n",
    "        parametros = busqueda.best_params_\n",
    "\n",
    "        # Evaluar el modelo en el conjunto de validación\n",
    "        y_val_pred = busqueda.predict_proba(X_val)[:, 1]\n",
    "        puntaje = roc_auc_score(y_val, y_val_pred)\n",
    "\n",
    "        with open(f'{nombre}.pkl', 'wb') as archivo:\n",
    "            pickle.dump({'modelo': mejor_modelo,\n",
    "                        'parametros': parametros,\n",
    "                        'puntaje': mejor_puntaje}, archivo)\n",
    "    \n",
    "        print(f\"Información guardada en {nombre}.pkl\")\n",
    "\n",
    "    resultados[nombre] = {\n",
    "        'mejores_params': parametros,\n",
    "        'roc_auc': puntaje\n",
    "    }\n",
    "\n",
    "    print(f\"Mejores hiperparámetros: {parametros}, ROC AUC (validación) = {puntaje:.4f}\")\n",
    "\n",
    "    # Guardar el mejor modelo\n",
    "    if puntaje > mejor_puntaje:\n",
    "        mejor_puntaje = puntaje\n",
    "        mejor_modelo = busqueda.best_estimator_\n",
    "\n",
    "print(f\"\\nModelo con el mejor rendimiento: {mejor_modelo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar el conjunto de prueba\n",
    "X_prueba_escalada = escalador.transform(df_prueba.drop(columns=['id']))\n",
    "\n",
    "# Predecir con el mejor modelo encontrado\n",
    "mejor_modelo.fit(X_entrenamiento, y_entrenamiento)\n",
    "y_prueba_pred_proba = busqueda.predict_proba(X_prueba_escalada)[:, 1]\n",
    "\n",
    "# Guardar los resultados en un archivo\n",
    "resultados = pd.DataFrame({'id': df_prueba['id'], 'engagement': y_prueba_pred_proba})\n",
    "resultados.to_csv('resultados.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
