{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de los datos de entrenamiento: (9239, 10)\n",
      "Dimensiones de los datos de prueba: (2309, 9)\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos\n",
    "df_entrenamiento = pd.read_csv('train.csv')\n",
    "df_prueba = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"Dimensiones de los datos de entrenamiento:\", df_entrenamiento.shape)\n",
    "print(\"Dimensiones de los datos de prueba:\", df_prueba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de los datos con los que se entrenará: (7391, 8)\n",
      "Dimensiones de los datos de validación: (1848, 8)\n"
     ]
    }
   ],
   "source": [
    "# Convertir participación a valor numérico\n",
    "df_entrenamiento['engagement'] = df_entrenamiento['engagement'].astype(int)\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X = df_entrenamiento.drop(columns=['id', 'engagement'])\n",
    "y = df_entrenamiento['engagement']\n",
    "\n",
    "# Normalizar las características\n",
    "escalador = StandardScaler()\n",
    "X_escalada = escalador.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y de validación\n",
    "X_entrenamiento, X_val, y_entrenamiento, y_val = train_test_split(X_escalada, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Dimensiones de los datos con los que se entrenará:\", X_entrenamiento.shape)\n",
    "print(\"Dimensiones de los datos de validación:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los modelos\n",
    "modelos = {\n",
    "    # 'Regresión logística': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    # 'k vecinos más cercanos': KNeighborsClassifier(),\n",
    "    # 'Clasificador de margen máximo': SVC(probability=True, random_state=42),\n",
    "    # 'Árbol de decisión': DecisionTreeClassifier(),\n",
    "    # 'Bosque aleatorio': RandomForestClassifier(random_state=42),\n",
    "    # 'Potencialización de gradiente': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    # 'LightGBM': LGBMClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la cuadrícula de hiperparámetros\n",
    "cuadriculas_params = {\n",
    "    # 'Regresión logística': [\n",
    "    #     {\n",
    "    #         'C': [0.01, 0.02, 0.05, 0.1, 1],\n",
    "    #         'penalty': ['l1', 'l2'],\n",
    "    #         'solver': ['liblinear', 'saga'],\n",
    "    #         'class_weight': [None, 'balanced']\n",
    "    #     },\n",
    "    #     {\n",
    "    #         'C': [0.01, 0.02, 0.05, 0.1, 1],\n",
    "    #         'penalty': ['elasticnet'],\n",
    "    #         'solver': ['saga'],\n",
    "    #         'class_weight': [None, 'balanced'],\n",
    "    #         'l1_ratio': [0.1, 0.5, 0.9]\n",
    "    #     }\n",
    "    # ],\n",
    "    # 'k vecinos más cercanos': {\n",
    "    #     'n_neighbors': [49, 51, 53, 55, 57, 59, 61],\n",
    "    #     'weights': ['distance'],\n",
    "    #     'p': [1, 2, 3, 4, 5],\n",
    "    #     'metric': ['minkowski'],\n",
    "    #     'algorithm': ['ball_tree']\n",
    "    # }\n",
    "    # 'Clasificador de margen máximo': [\n",
    "    #     {\n",
    "    #         'C': [0.002, 0.005, 0.01, 0.02, 0.05],\n",
    "    #         'kernel': ['rbf'],\n",
    "    #         'gamma': ['scale', 'auto', 0.002, 0.005, 0.01, 0.02, 0.05]\n",
    "    #     },\n",
    "    #     {\n",
    "    #         'C': [0.001, 0.01, 0.1, 1.0],\n",
    "    #         'kernel': ['sigmoid'],\n",
    "    #         'gamma': ['scale', 'auto', 0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    #         'coef0': [0.01, 0.1, 1.0]\n",
    "    #     },\n",
    "    #     {\n",
    "    #         'C': [1, 2, 3],\n",
    "    #         'kernel': ['poly'],\n",
    "    #         'gamma': [0.04, 0.05, 0.1],\n",
    "    #         'degree': [3],\n",
    "    #         'coef0': [10.0, 13.0, 19.0]\n",
    "    #     }\n",
    "    # ]\n",
    "    # 'Árbol de decisión': {\n",
    "    #     'max_depth': [10, None],\n",
    "    #     'min_samples_split': [5, 6],\n",
    "    #     'min_samples_leaf': [2, 4, 5],\n",
    "    #     'max_features': [0.5, 0.8, None],\n",
    "    #     'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    #     'splitter': ['best', 'random'],\n",
    "    #     'class_weight': ['balanced', None],\n",
    "    #     'max_leaf_nodes': [30, 40, None],\n",
    "    #     'min_weight_fraction_leaf': [0.0, 0.01],\n",
    "    #     'min_impurity_decrease': [0.0, 0.01, 0.02]\n",
    "    # }\n",
    "    # 'Bosque aleatorio': {\n",
    "    #     'n_estimators': [200],\n",
    "    #     'max_depth': [15],\n",
    "    #     'min_samples_split': [5],\n",
    "    #     'min_samples_leaf': [2],\n",
    "    #     'max_features': ['log2'],\n",
    "    #     'class_weight': ['balanced'],\n",
    "    #     'criterion': ['gini', 'entropy']\n",
    "    # },\n",
    "    # 'Potencialización de gradiente': {\n",
    "    #     'n_estimators': [100],\n",
    "    #     'max_depth': [5],\n",
    "    #     'learning_rate': [0.1],\n",
    "    #     'min_samples_split': [2],\n",
    "    #     'min_samples_leaf': [2],\n",
    "    #     'criterion': ['friedman_mse'],\n",
    "    #     'loss': ['log_loss', 'exponential'],\n",
    "    #     'min_weight_fraction_leaf': [0.02, 0.03]\n",
    "    # },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': [5],\n",
    "        'learning_rate': [0.1],\n",
    "        'min_child_weight': [5],\n",
    "        'subsample': [1.0],\n",
    "        'gamma': [0.1],\n",
    "        'reg_lambda': [0, 1],\n",
    "        'objective': ['binary:logistic', 'binary:hinge', 'binary:logitraw'],\n",
    "        'monotone_constraints': [(1, -1)],\n",
    "        'interaction_constraints': ['[[0, 1], [2, 3, 4]]'],\n",
    "    },\n",
    "    # 'LightGBM': {\n",
    "    #     'n_estimators': [100, 200],\n",
    "    #     'max_depth': [-1, 5, 10],\n",
    "    #     'learning_rate': [0.01, 0.1, 0.2],\n",
    "    #     'num_leaves': [20, 40, 60],\n",
    "    #     'min_child_samples': [20, 30, 50]\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: XGBoost\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV 2/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logistic, reg_lambda=0, subsample=1.0;, score=0.858 total time=   0.1s\n",
      "[CV 1/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logistic, reg_lambda=0, subsample=1.0;, score=0.867 total time=   0.1s\n",
      "[CV 4/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logistic, reg_lambda=0, subsample=1.0;, score=0.901 total time=   0.1s\n",
      "[CV 5/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logistic, reg_lambda=0, subsample=1.0;, score=0.883 total time=   0.1s\n",
      "[CV 2/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:hinge, reg_lambda=1, subsample=1.0;, score=0.669 total time=   0.1s\n",
      "[CV 4/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logistic, reg_lambda=1, subsample=1.0;, score=0.902 total time=   0.1s\n",
      "[CV 1/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logistic, reg_lambda=1, subsample=1.0;, score=0.864 total time=   0.1s\n",
      "[CV 4/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:hinge, reg_lambda=0, subsample=1.0;, score=0.709 total time=   0.1s\n",
      "[CV 2/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logistic, reg_lambda=1, subsample=1.0;, score=0.857 total time=   0.1s[CV 3/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:hinge, reg_lambda=1, subsample=1.0;, score=0.694 total time=   0.1s\n",
      "\n",
      "[CV 3/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logistic, reg_lambda=0, subsample=1.0;, score=0.865 total time=   0.2s\n",
      "[CV 1/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:hinge, reg_lambda=1, subsample=1.0;, score=0.675 total time=   0.2s\n",
      "[CV 4/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:hinge, reg_lambda=1, subsample=1.0;, score=0.699 total time=   0.2s\n",
      "[CV 3/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:hinge, reg_lambda=0, subsample=1.0;, score=0.697 total time=   0.2s\n",
      "[CV 1/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:hinge, reg_lambda=0, subsample=1.0;, score=0.674 total time=   0.2s\n",
      "[CV 5/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logistic, reg_lambda=1, subsample=1.0;, score=0.883 total time=   0.2s\n",
      "[CV 2/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:hinge, reg_lambda=0, subsample=1.0;, score=0.672 total time=   0.2s\n",
      "[CV 3/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logistic, reg_lambda=1, subsample=1.0;, score=0.864 total time=   0.2s\n",
      "[CV 5/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:hinge, reg_lambda=0, subsample=1.0;, score=0.688 total time=   0.2s\n",
      "[CV 5/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:hinge, reg_lambda=1, subsample=1.0;, score=0.694 total time=   0.2s\n",
      "[CV 1/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logitraw, reg_lambda=0, subsample=1.0;, score=0.867 total time=   0.1s\n",
      "[CV 2/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logitraw, reg_lambda=0, subsample=1.0;, score=0.858 total time=   0.1s\n",
      "[CV 3/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logitraw, reg_lambda=0, subsample=1.0;, score=0.865 total time=   0.1s\n",
      "[CV 5/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logitraw, reg_lambda=0, subsample=1.0;, score=0.883 total time=   0.1s\n",
      "[CV 4/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logitraw, reg_lambda=0, subsample=1.0;, score=0.901 total time=   0.1s\n",
      "[CV 1/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logitraw, reg_lambda=1, subsample=1.0;, score=0.864 total time=   0.1s\n",
      "[CV 2/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logitraw, reg_lambda=1, subsample=1.0;, score=0.857 total time=   0.1s\n",
      "[CV 4/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logitraw, reg_lambda=1, subsample=1.0;, score=0.902 total time=   0.1s\n",
      "[CV 3/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logitraw, reg_lambda=1, subsample=1.0;, score=0.864 total time=   0.1s\n",
      "[CV 5/5] END gamma=0.1, interaction_constraints=[[0, 1], [2, 3, 4]], learning_rate=0.1, max_depth=5, min_child_weight=5, monotone_constraints=(1, -1), n_estimators=100, objective=binary:logitraw, reg_lambda=1, subsample=1.0;, score=0.883 total time=   0.1s\n",
      "Mejores hiperparámetros: {'gamma': 0.1, 'interaction_constraints': '[[0, 1], [2, 3, 4]]', 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 5, 'monotone_constraints': (1, -1), 'n_estimators': 100, 'objective': 'binary:logistic', 'reg_lambda': 0, 'subsample': 1.0}, ROC AUC (validación) = 0.9005\n",
      "\n",
      "Modelo con el mejor rendimiento: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=0.1, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints='[[0, 1], [2, 3, 4]]', learning_rate=0.1,\n",
      "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "              min_child_weight=5, missing=nan, monotone_constraints=(1, -1),\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...)\n"
     ]
    }
   ],
   "source": [
    "# Evaluar los modelos\n",
    "mejor_puntaje = -float('inf')\n",
    "mejor_modelo = None\n",
    "resultados = {}\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    print(\"Modelo:\", nombre)\n",
    "\n",
    "    # Validación cruzada con el criterio de área bajo la curva característica operativa del receptor (ROC AUC)\n",
    "    busqueda = GridSearchCV(estimator=modelo, param_grid=cuadriculas_params[nombre], cv=5, scoring='roc_auc', n_jobs=-1, verbose=3)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    busqueda.fit(X_entrenamiento, y_entrenamiento)\n",
    "\n",
    "    # Obtener los mejores hiperparámetros\n",
    "    mejores_params = busqueda.best_params_\n",
    "\n",
    "    # Evaluar el modelo en el conjunto de validación\n",
    "    y_val_pred = busqueda.predict_proba(X_val)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred)\n",
    "\n",
    "    resultados[nombre] = {\n",
    "        'mejores_params': mejores_params,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "\n",
    "    print(f\"Mejores hiperparámetros: {mejores_params}, ROC AUC (validación) = {roc_auc:.4f}\")\n",
    "\n",
    "    # Guardar el mejor modelo\n",
    "    if roc_auc > mejor_puntaje:\n",
    "        mejor_puntaje = roc_auc\n",
    "        mejor_modelo = busqueda.best_estimator_\n",
    "\n",
    "print(f\"\\nModelo con el mejor rendimiento: {mejor_modelo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar el conjunto de prueba\n",
    "X_prueba_escalada = escalador.transform(df_prueba.drop(columns=['id']))\n",
    "\n",
    "# Predecir con el mejor modelo encontrado\n",
    "mejor_modelo.fit(X_entrenamiento, y_entrenamiento)\n",
    "y_prueba_pred_proba = busqueda.predict_proba(X_prueba_escalada)[:, 1]\n",
    "\n",
    "# Guardar los resultados en un archivo\n",
    "resultados = pd.DataFrame({'id': df_prueba['id'], 'engagement': y_prueba_pred})\n",
    "resultados.to_csv('resultados.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
